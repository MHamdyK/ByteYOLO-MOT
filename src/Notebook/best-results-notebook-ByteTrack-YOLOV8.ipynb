{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":89993,"databundleVersionId":10699058,"sourceType":"competition"},{"sourceId":9574582,"sourceType":"datasetVersion","datasetId":5836663}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## yolov12 fine-tuning with MOT17 + Compition dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting notebook\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install required packages\n!pip install -q ultralytics\n!pip install -q atomicwrites","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Change working directory\n%cd /kaggle/working\n\n# Clone the ByteTrack repository and install it\n!git clone https://github.com/ifzhang/ByteTrack.git\n%cd /kaggle/working/ByteTrack\n!sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt\n\n!pip install -q -r requirements.txt\n!python setup.py -q develop\n!pip install -q cython_bbox onemetric loguru lap thop\n# Clean up notebook output\nfrom IPython.display import clear_output\nclear_output()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List the ByteTrack directory to check installation\n!ls /kaggle/working/ByteTrack","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Apply numpy fix PERMANENTLY\n!sed -i \"s/\\.astype(float32)/.astype(np.float32)/g\" /kaggle/working/ByteTrack/yolox/utils/visualize.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!find /kaggle/working/ByteTrack -type f -name \"*.py\" -exec sed -i -E \"s/([^n])p\\.(float32|float64|int32)/\\1np.\\2/g\" {} \\;","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep \"astype(np.float32)\" /kaggle/working/ByteTrack/yolox/utils/visualize.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport numpy as np  \n# Patch for deprecated np.float in numpy 1.20+\nif not hasattr(np, 'float'):\n    np.float = float\n\nsys.path.insert(0, '/kaggle/working/ByteTrack')\nif 'yolox' in sys.modules:\n    del sys.modules['yolox']\nfrom yolox.tracker.byte_tracker import BYTETracker, STrack\nprint(\"âœ… Modules imported successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import yolox\nprint(\"yolox.__version__:\", yolox.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, sys, cv2, pandas as pd, torch, locale, shutil, glob, json\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom ultralytics import YOLO\nfrom dataclasses import dataclass\nfrom atomicwrites import atomic_write\nfrom pathlib import Path\nlocale.getpreferredencoding = lambda: \"UTF-8\"\nfrom onemetric.cv.utils.iou import box_iou_batch\nfrom typing import List, Dict, Tuple, Optional, Union\n#TPU\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Define Unified Dataset Directories\n# ================================\nOUTPUT_DIR = \"/kaggle/working/mot_output\"\nYOLO_TRAIN_DATA_DIR = f\"{OUTPUT_DIR}/yolo_data\"\nSUBMISSION_FILE = f\"{OUTPUT_DIR}/submission.csv\"\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(YOLO_TRAIN_DATA_DIR, exist_ok=True)\nos.makedirs(f\"{YOLO_TRAIN_DATA_DIR}/images/train\", exist_ok=True)\nos.makedirs(f\"{YOLO_TRAIN_DATA_DIR}/images/val\", exist_ok=True)\nos.makedirs(f\"{YOLO_TRAIN_DATA_DIR}/labels/train\", exist_ok=True)\nos.makedirs(f\"{YOLO_TRAIN_DATA_DIR}/labels/val\", exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define Dataset Roots and Sequences\n# ================================\n# Competition dataset\nCOMP_DATASET_ROOT = \"/kaggle/input/surveillance-for-retail-stores/tracking\"\n# MOT17 dataset root\nMOT17_ROOT = \"/kaggle/input/mot-17/MOT17/train\"\n\n# For the competition dataset, we use all available sequences for training.\nCOMP_SEQUENCES = [\"02\", \"03\", \"05\"]\n\n# MOT17 split: use some sequences for training and others for validation.\nMOT17_TRAIN_SEQUENCES = [\"MOT17-02-SDP\", \"MOT17-04-SDP\", \"MOT17-05-SDP\", \"MOT17-09-SDP\",\"MOT17-10-SDP\"]\nMOT17_VAL_SEQUENCES   = [\"MOT17-11-SDP\", \"MOT17-13-SDP\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5: Dataset Preparation Functions (Full Updated Version)\n\ndef safe_bbox_conversion(x: float, y: float, w: float, h: float, \n                        img_w: int, img_h: int) -> Tuple[float, float, float, float]:\n    \"\"\"Convert MOT bbox to YOLO format with safety checks\"\"\"\n    x_center = max(0.0, min(1.0, (x + w/2) / img_w))\n    y_center = max(0.0, min(1.0, (y + h/2) / img_h))\n    w_norm = max(0.001, min(1.0, w / img_w))  # Prevent zero-width\n    h_norm = max(0.001, min(1.0, h / img_h))  # Prevent zero-height\n    return (round(x_center, 6), round(y_center, 6), \n            round(w_norm, 6), round(h_norm, 6))\n\ndef validate_mot_annotation(ann: dict, gt_file: Path, frame_id: int) -> bool:\n    \"\"\"Validate MOT annotation format\"\"\"\n    valid = True\n    if ann['x'] < 0 or ann['y'] < 0:\n        print(f\"âš ï¸ Invalid X/Y in {gt_file} frame {frame_id}: {ann}\")\n        valid = False\n    if ann['width'] <= 0 or ann['height'] <= 0:\n        print(f\"âš ï¸ Invalid W/H in {gt_file} frame {frame_id}: {ann}\")\n        valid = False\n    if ann['conf'] < 0 or ann['conf'] > 1:\n        print(f\"âš ï¸ Invalid confidence in {gt_file} frame {frame_id}: {ann}\")\n        valid = False\n    return valid\n\ndef read_mot_gt(gt_file: Path, dataset_type=\"competition\") -> list:\n    \"\"\"\n    Read and validate MOT ground truth file.\n    For competition: only include annotations with class_id == 1 and visibility > 0.1.\n    For MOT17: include annotations with track_id > 0 and visibility > 0.1 (remap to single pedestrian class).\n    \"\"\"\n    annotations = []\n    try:\n        with gt_file.open('r') as f:\n            for line_num, line in enumerate(f, 1):\n                parts = line.strip().split(',')\n                if len(parts) < 9:\n                    continue\n                try:\n                    frame_id = int(float(parts[0]))\n                    track_id = int(float(parts[1]))\n                    x = float(parts[2])\n                    y = float(parts[3])\n                    width = float(parts[4])\n                    height = float(parts[5])\n                    conf = float(parts[6])\n                    class_id = int(float(parts[7]))\n                    visibility = float(parts[8])\n                    if dataset_type == \"competition\":\n                        if class_id == 1 and visibility >= 0.06:\n                            ann = {\n                                'frame_id': frame_id,\n                                'track_id': track_id,\n                                'x': x,\n                                'y': y,\n                                'width': width,\n                                'height': height,\n                                'conf': conf\n                            }\n                            annotations.append(ann)\n                    elif dataset_type == \"mot17\":\n                        # For MOT17, consider valid if track_id > 0 and visibility > 0.1\n                        if track_id > 0 and visibility > 0.1:\n                            ann = {\n                                'frame_id': frame_id,\n                                'track_id': track_id,\n                                'x': x,\n                                'y': y,\n                                'width': width,\n                                'height': height,\n                                'conf': conf\n                            }\n                            annotations.append(ann)\n                except Exception as e:\n                    print(f\"ðŸš¨ Error in {gt_file} line {line_num}: {str(e)}\")\n    except Exception as e:\n        print(f\"ðŸ”¥ Failed to process {gt_file}: {str(e)}\")\n    return annotations\n\n\ndef prepare_competition_dataset():\n    print(\"ðŸš€ Preparing Competition Dataset...\")\n    comp_base = Path(COMP_DATASET_ROOT) / \"train\"\n    yolo_path = Path(YOLO_TRAIN_DATA_DIR)\n    frames_to_annotations = defaultdict(list)\n    \n    for seq in COMP_SEQUENCES:\n        seq_path = comp_base / seq\n        gt_file = seq_path / \"gt\" / \"gt.txt\"\n        if not gt_file.exists():\n            raise FileNotFoundError(f\"ðŸš¨ Missing GT file: {gt_file}\")\n        annotations = read_mot_gt(gt_file, dataset_type=\"competition\")\n        for ann in annotations:\n            key = (seq, ann['frame_id'])\n            frames_to_annotations[key].append(ann)\n    \n    # Process all competition frames into the training folder\n    for (seq, frame_id), anns in tqdm(frames_to_annotations.items(), desc=\"Processing Competition Frames\"):\n        img_path = comp_base / seq / \"img1\" / f\"{frame_id:06d}.jpg\"\n        if not img_path.exists():\n            print(f\"Missing image: {img_path}\")\n            continue\n        img = cv2.imread(str(img_path))\n        if img is None:\n            print(f\"Invalid image: {img_path}\")\n            continue\n        height, width = img.shape[:2]\n        dest_img = yolo_path / \"images/train\" / f\"comp_{seq}_{frame_id:06d}.jpg\"\n        dest_label = yolo_path / \"labels/train\" / f\"comp_{seq}_{frame_id:06d}.txt\"\n        with atomic_write(dest_img, mode='wb', overwrite=True) as f:\n            f.write(img_path.read_bytes())\n        label_content = []\n        for ann in anns:\n            try:\n                xc, yc, w_norm, h_norm = safe_bbox_conversion(ann['x'], ann['y'], ann['width'], ann['height'], width, height)\n                label_content.append(f\"0 {xc} {yc} {w_norm} {h_norm}\")\n            except Exception as e:\n                print(f\"âš ï¸ Invalid bbox in comp {seq}-{frame_id}: {str(e)}\")\n        if label_content:\n            with atomic_write(dest_label, mode='w', overwrite=True) as f:\n                f.write(\"\\n\".join(label_content))\n        else:\n            os.remove(dest_img)\n    print(\"âœ… Competition dataset prepared.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_mot17_dataset():\n    print(\"ðŸš€ Preparing MOT17 Dataset...\")\n    mot17_base = Path(MOT17_ROOT)\n    yolo_path = Path(YOLO_TRAIN_DATA_DIR)\n    \n    # Process MOT17 training sequences\n    for seq in MOT17_TRAIN_SEQUENCES:\n        seq_path = mot17_base / seq\n        gt_file = seq_path / \"gt\" / \"gt.txt\"\n        if not gt_file.exists():\n            print(f\"ðŸš¨ Missing GT file: {gt_file}\")\n            continue\n        annotations = read_mot_gt(gt_file, dataset_type=\"mot17\")\n        frames_to_annotations = defaultdict(list)\n        for ann in annotations:\n            key = (seq, ann['frame_id'])\n            frames_to_annotations[key].append(ann)\n        for (seq_id, frame_id), anns in tqdm(frames_to_annotations.items(), desc=f\"Processing MOT17 Train {seq}\"):\n            img_path = seq_path / \"img1\" / f\"{frame_id:06d}.jpg\"\n            if not img_path.exists():\n                print(f\"Missing image: {img_path}\")\n                continue\n            img = cv2.imread(str(img_path))\n            if img is None:\n                print(f\"Invalid image: {img_path}\")\n                continue\n            height, width = img.shape[:2]\n            dest_img = yolo_path / \"images/train\" / f\"mot17_{seq}_{frame_id:06d}.jpg\"\n            dest_label = yolo_path / \"labels/train\" / f\"mot17_{seq}_{frame_id:06d}.txt\"\n            with atomic_write(dest_img, mode='wb', overwrite=True) as f:\n                f.write(img_path.read_bytes())\n            label_content = []\n            for ann in anns:\n                try:\n                    xc, yc, w_norm, h_norm = safe_bbox_conversion(ann['x'], ann['y'], ann['width'], ann['height'], width, height)\n                    label_content.append(f\"0 {xc} {yc} {w_norm} {h_norm}\")\n                except Exception as e:\n                    print(f\"âš ï¸ Invalid bbox in MOT17 train {seq}-{frame_id}: {str(e)}\")\n            if label_content:\n                with atomic_write(dest_label, mode='w', overwrite=True) as f:\n                    f.write(\"\\n\".join(label_content))\n            else:\n                os.remove(dest_img)\n                \n    # Process MOT17 validation sequences\n    for seq in MOT17_VAL_SEQUENCES:\n        seq_path = mot17_base / seq\n        gt_file = seq_path / \"gt\" / \"gt.txt\"\n        if not gt_file.exists():\n            print(f\"ðŸš¨ Missing GT file: {gt_file}\")\n            continue\n        annotations = read_mot_gt(gt_file, dataset_type=\"mot17\")\n        frames_to_annotations = defaultdict(list)\n        for ann in annotations:\n            key = (seq, ann['frame_id'])\n            frames_to_annotations[key].append(ann)\n        for (seq_id, frame_id), anns in tqdm(frames_to_annotations.items(), desc=f\"Processing MOT17 Val {seq}\"):\n            img_path = seq_path / \"img1\" / f\"{frame_id:06d}.jpg\"\n            if not img_path.exists():\n                print(f\"Missing image: {img_path}\")\n                continue\n            img = cv2.imread(str(img_path))\n            if img is None:\n                print(f\"Invalid image: {img_path}\")\n                continue\n            height, width = img.shape[:2]\n            dest_img = yolo_path / \"images/val\" / f\"mot17_{seq}_{frame_id:06d}.jpg\"\n            dest_label = yolo_path / \"labels/val\" / f\"mot17_{seq}_{frame_id:06d}.txt\"\n            with atomic_write(dest_img, mode='wb', overwrite=True) as f:\n                f.write(img_path.read_bytes())\n            label_content = []\n            for ann in anns:\n                try:\n                    xc, yc, w_norm, h_norm = safe_bbox_conversion(ann['x'], ann['y'], ann['width'], ann['height'], width, height)\n                    label_content.append(f\"0 {xc} {yc} {w_norm} {h_norm}\")\n                except Exception as e:\n                    print(f\"âš ï¸ Invalid bbox in MOT17 val {seq}-{frame_id}: {str(e)}\")\n            if label_content:\n                with atomic_write(dest_label, mode='w', overwrite=True) as f:\n                    f.write(\"\\n\".join(label_content))\n            else:\n                os.remove(dest_img)\n    print(\"âœ… MOT17 dataset prepared.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dataset_yaml():\n    yaml_content = f\"\"\"train: {YOLO_TRAIN_DATA_DIR}/images/train\nval: {YOLO_TRAIN_DATA_DIR}/images/val\nnc: 1\nnames: ['pedestrian']\n\"\"\"\n    dataset_yaml_path = Path(YOLO_TRAIN_DATA_DIR) / \"dataset.yaml\"\n    with atomic_write(dataset_yaml_path, overwrite=True) as f:\n        f.write(yaml_content)\n    print(\"âœ… dataset.yaml created.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prepare_competition_dataset()\nprepare_mot17_dataset()\ncreate_dataset_yaml()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.manual_seed(42)\ntorch.manual_seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7: YOLOv12 Training Function\ndef train_yolov11():\n    \"\"\"Train YOLOv11 model on the prepared dataset\"\"\"\n    print(\"Training YOLOv11 model...\")\n    \n    # Initialize YOLOv12 model (smaller model for faster training)\n    model = YOLO('yolov8m.pt')\n    \n    # Check for CUDA availability\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    #TPU\n    # device = xm.xla_device()\n\n    \n    # Start training\n    results = model.train(\n    data=f\"{YOLO_TRAIN_DATA_DIR}/dataset.yaml\",\n    epochs=2,              # Train longer for MOT's complexity\n    patience=4,\n    imgsz=1184,              # Critical for small pedestrians\n    batch=10,                # Max batch your GPU allows\n    optimizer='SGD',         # AdamW is suboptimal for detection\n    lr0=0.005,               # Higher initial LR\n    lrf=0.015,               # Final LR factor\n    momentum=0.937,\n    weight_decay=0.0005,\n    cos_lr=True,             # Learning rate decay\n    warmup_epochs=2,         # Stabilize training\n    # Augmentations\n    augment=True,\n    fliplr=0.5,              # Safe for pedestrians\n    hsv_h=0.015,             # Minimal hue shift (preserve clothing colors)\n    hsv_s=0.4,               # Reduce saturation jitter\n    hsv_v=0.2,               # Minimal brightness change (for low-light prep)\n    degrees=0.0,             # Avoid rotation (pedestrians are upright)\n    translate=0.05,          # Small translation\n    scale=0.2,               # Limited scaling\n    erasing=0.1,             # Reduced occlusion sim\n    mixup=0.0,               # Disable (distorts pedestrian interactions)\n    copy_paste=0.0,          # Disable (unrealistic for tracking)\n    auto_augment='randaugment',       # Disable RandAugment\n    # Tracking-Specific\n    overlap_mask=False,      # Critical for MOT\n    single_cls=True,         # Focus on \"person\" class only\n    pretrained=True,# Start from COCO weights\n    device = device,\n    half = True\n)\n    \n    print(\"Training completed!\")\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: Run Training\n# Train the model (can be run separately)\ntrained_model = train_yolov11()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: ByteTrack Helper\n# ByteTrack configuration\n@dataclass(frozen=True)\nclass BYTETrackerArgs:\n    track_thresh: float = 0.0\n    track_buffer: int = 30\n    match_thresh: float = 0.7\n    aspect_ratio_thresh: float = 3.0\n    min_box_area: float = 1.0\n    mot20: bool = True\n\n# Helper functions for tracking\ndef detections2boxes(xyxy, confidence) -> np.ndarray:\n    \"\"\"Convert detections to format that can be used by ByteTrack\"\"\"\n    return np.hstack((\n        xyxy,\n        confidence[:, np.newaxis]\n    ))\n\ndef tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n    \"\"\"Convert STrack objects to bounding boxes\"\"\"\n    return np.array([\n        track.tlbr\n        for track\n        in tracks\n    ], dtype=float)\n\ndef match_detections_with_tracks(xyxy, tracks: List[STrack]) -> List[Optional[int]]:\n    \"\"\"Match detections with existing tracks\"\"\"\n    if len(xyxy) == 0 or len(tracks) == 0:\n        return [None] * len(xyxy)\n\n    tracks_boxes = tracks2boxes(tracks=tracks)\n    iou = box_iou_batch(tracks_boxes, xyxy)\n    track2detection = np.argmax(iou, axis=1)\n\n    tracker_ids = [None] * len(xyxy)\n\n    for tracker_index, detection_index in enumerate(track2detection):\n        if iou[tracker_index, detection_index] != 0:\n            tracker_ids[detection_index] = tracks[tracker_index].track_id\n\n    return tracker_ids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"assert Path(\"/kaggle/working/ByteTrack/runs/detect/train2/weights/best.pt\").exists()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_tracking_inference():\n    print(\"Running tracking inference on test sequence...\")\n    weights_dir = Path(\"/kaggle/working/ByteTrack/runs/detect/train2/weights\")\n    try:\n        best_model_path = next(weights_dir.glob(\"best*.pt\"))\n        model = YOLO(str(best_model_path))\n        print(f\"âœ… Loaded best model from: {best_model_path}\")\n    except StopIteration:\n        raise FileNotFoundError(f\"âŒ No trained weights found in {weights_dir}\")\n\n    # Using the competition dataset test sequence\n    test_img_dir = Path(f\"{COMP_DATASET_ROOT}/test/01/img1\")\n    test_seq_info = Path(f\"{COMP_DATASET_ROOT}/test/01/seqinfo.ini\")\n    if not test_img_dir.exists():\n        raise FileNotFoundError(f\"Test image directory {test_img_dir} not found\")\n\n    seq_info = {}\n    with test_seq_info.open() as f:\n        for line in f:\n            if '=' in line:\n                key, value = line.strip().split('=')\n                seq_info[key.strip()] = value.strip()\n\n    try:\n        frame_rate = int(seq_info['frameRate'])\n        num_frames = int(seq_info['seqLength'])\n    except KeyError as e:\n        raise ValueError(f\"Missing key in seqinfo.ini: {e}\")\n\n    print(f\"ðŸ“Š Test sequence: {num_frames} frames @ {frame_rate}FPS\")\n\n    tracker_args = BYTETrackerArgs(\n        track_thresh=0.35,\n        track_buffer=45,\n        match_thresh=0.7\n    )\n    byte_tracker = BYTETracker(tracker_args)\n    submission_data = []\n    \n    for frame_idx in tqdm(range(1, num_frames + 1), desc=\"Tracking\"):\n        img_path = test_img_dir / f\"{frame_idx:06d}.jpg\"\n        frame_data = {\n            \"ID\": frame_idx - 1,\n            \"Frame\": frame_idx,\n            \"Objects\": [],\n            \"Objective\": \"tracking\"\n        }\n        if not img_path.exists():\n            submission_data.append(frame_data)\n            continue\n\n        try:\n            results = model.predict(str(img_path), conf=0.3, iou=0.7, verbose=False)[0]\n            if len(results.boxes) == 0:\n                submission_data.append(frame_data)\n                continue\n\n            xyxy = results.boxes.xyxy.cpu().numpy().astype(np.float64)\n            img = cv2.imread(str(img_path))\n            if img is None:\n                raise ValueError(f\"Failed to read image {img_path}\")\n            detections = np.hstack([xyxy, np.ones((len(xyxy), 1))])  # Force confidence = 1\n            tracks = byte_tracker.update(\n                output_results=detections,\n                img_info=img.shape[:2],\n                img_size=img.shape[:2]\n            )\n            track_ids = match_detections_with_tracks(xyxy, tracks)\n            objects = []\n            for i, (box, track_id) in enumerate(zip(xyxy, track_ids)):\n                if track_id is not None:\n                    obj = {\n                        \"tracked_id\": int(track_id),\n                        \"x\": float(box[0]),\n                        \"y\": float(box[1]),\n                        \"w\": float(box[2] - box[0]),\n                        \"h\": float(box[3] - box[1]),\n                        \"confidence\": 1.0\n                    }\n                    objects.append(obj)\n            frame_data[\"Objects\"] = objects\n            submission_data.append(frame_data)\n        except Exception as e:\n            print(f\"âš ï¸ Error processing frame {frame_idx}: {str(e)}\")\n            submission_data.append(frame_data)\n\n    submission_df = pd.DataFrame(submission_data)\n    required_columns = {\"ID\", \"Frame\", \"Objects\", \"Objective\"}\n    if not required_columns.issubset(submission_df.columns):\n        missing = required_columns - set(submission_df.columns)\n        raise ValueError(f\"Missing submission columns: {missing}\")\n\n    submission_df.to_csv(SUBMISSION_FILE, index=False)\n    print(f\"âœ… Submission saved to {SUBMISSION_FILE}\")\n    return submission_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10: Run Inference and Generate Submissionrrrr\n# Run inference and create submission file\nsubmission = run_tracking_inference()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_tracking_results(num_frames=5):\n    \"\"\"Visualize a sample of tracking results using the unified directories\"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.patches import Rectangle\n    import json\n\n    # Load submission data (assumes the submission file was saved at SUBMISSION_FILE)\n    submission_df = pd.read_csv(SUBMISSION_FILE)\n\n    # Use the competition test sequence directory from COMP_DATASET_ROOT and TEST_SEQUENCE\n    # (If TEST_SEQUENCE isn't defined globally, we define it here)\n    TEST_SEQUENCE = \"01\"\n    test_img_dir = f\"{COMP_DATASET_ROOT}/test/{TEST_SEQUENCE}/img1\"\n\n    # Choose random frames with objects based on the \"Objects\" column.\n    # The submission file uses column names \"Frame\" and \"Objects\" (capitalized).\n    frames_with_objects = submission_df[submission_df['Objects'].apply(lambda x: len(eval(x)) > 0)]['Frame'].values\n    if len(frames_with_objects) == 0:\n        print(\"No frames with tracked objects found.\")\n        return\n\n    # Randomly select a few frames to visualize\n    frames_to_show = np.random.choice(frames_with_objects, min(num_frames, len(frames_with_objects)), replace=False)\n    \n    plt.figure(figsize=(15, 5 * len(frames_to_show)))\n    track_history = {}  # Dictionary to store track center history for each track ID\n    \n    for i, frame_num in enumerate(frames_to_show):\n        # Construct image path using our competition test directory\n        img_path = f\"{test_img_dir}/{int(frame_num):06d}.jpg\"\n        if not os.path.exists(img_path):\n            continue\n        img = cv2.imread(img_path)\n        if img is None:\n            continue\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Get the corresponding row in submission_df for this frame\n        frame_data = submission_df[submission_df['Frame'] == frame_num].iloc[0]\n        # Convert the \"Objects\" column from its string representation to a list\n        try:\n            objects = json.loads(frame_data['Objects'])\n        except Exception as e:\n            # Fall back to eval() if needed\n            objects = eval(frame_data['Objects'])\n        \n        # Plot the image\n        plt.subplot(len(frames_to_show), 1, i+1)\n        plt.imshow(img)\n        ax = plt.gca()\n        \n        for obj in objects:\n            # Assuming each object is stored as a dictionary with keys: \n            # \"tracked_id\", \"x\", \"y\", \"w\", \"h\", \"confidence\"\n            track_id = obj.get(\"tracked_id\")\n            x = obj.get(\"x\")\n            y = obj.get(\"y\")\n            w = obj.get(\"w\")\n            h = obj.get(\"h\")\n            \n            # Choose a color based on track ID (50 distinct colors)\n            colors = plt.cm.hsv(np.linspace(0, 1, 50))\n            color = colors[int(track_id) % 50]\n            \n            # Draw rectangle around the object\n            rect = Rectangle((x, y), w, h, edgecolor=color, facecolor='none', lw=2)\n            ax.add_patch(rect)\n            plt.text(x, y - 10, f\"ID: {track_id}\", color='white', fontsize=12,\n                     bbox=dict(facecolor='red', alpha=0.7))\n            \n            # Update track history for trail visualization\n            center = (x + w/2, y + h/2)\n            if track_id in track_history:\n                track_history[track_id].append(center)\n            else:\n                track_history[track_id] = [center]\n            \n            # If we have a history, plot the track trail\n            if len(track_history[track_id]) > 1:\n                trail = np.array(track_history[track_id])\n                plt.plot(trail[:, 0], trail[:, 1], color=color, alpha=0.5)\n        \n        plt.title(f\"Frame {int(frame_num)}\")\n        plt.axis('off')\n    \n    plt.tight_layout()\n    vis_path = f\"{OUTPUT_DIR}/tracking_visualization.png\"\n    plt.savefig(vis_path)\n    plt.show()\n    print(f\"Visualization saved to {vis_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Uncomment to run visualization\nvisualize_tracking_results()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12: Run Complete Pipeline\ndef run_complete_pipeline():\n    \"\"\"Run the complete pipeline from dataset preparation to submission\"\"\"\n    # 1. Prepare dataset\n    prepare_yolo_dataset()\n    \n    # 2. Train model\n    train_yolov12()\n    \n    # 3. Run inference and generate submission\n    run_tracking_inference()\n    \n    print(\"Complete pipeline executed successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Uncomment to run the complete pipeline at once\n# run_complete_pipeline()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_tracking_video(output_video_path=\"/kaggle/working/tracking_output.mp4\", fps=30):\n    \"\"\"\n    Process all images in COMP_DATASET_ROOT/test/01/img1 using YOLO and ByteTrack,\n    draw bounding boxes and track IDs, and write the processed frames to a video.\n    \n    Args:\n        output_video_path (str): Path to save the output video.\n        fps (int): Frames per second for the output video.\n    \"\"\"\n    # Define test image directory\n    test_img_dir = Path(f\"{COMP_DATASET_ROOT}/test/01/img1\")\n    \n    # Get sorted list of image paths\n    img_paths = sorted(list(test_img_dir.glob(\"*.jpg\")))\n    if not img_paths:\n        raise FileNotFoundError(f\"No images found in {test_img_dir}\")\n    \n    # Read the first image to get frame dimensions\n    first_img = cv2.imread(str(img_paths[0]))\n    if first_img is None:\n        raise ValueError(\"Could not read the first image.\")\n    height, width = first_img.shape[:2]\n    \n    # Initialize VideoWriter (using MP4V codec)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n    print(f\"Writing video to {output_video_path} at {fps} FPS, resolution: {width}x{height}\")\n    \n    # Initialize ByteTrack tracker with your configuration (adjust parameters as needed)\n    tracker_args = BYTETrackerArgs(\n        track_thresh=0.35,\n        track_buffer=45,\n        match_thresh=0.7,\n        aspect_ratio_thresh=3.0,\n        min_box_area=1.0,\n        mot20=True\n    )\n    byte_tracker = BYTETracker(tracker_args)\n    \n    # Process each frame\n    for img_path in tqdm(img_paths, desc=\"Creating video with tracking\"):\n        img = cv2.imread(str(img_path))\n        if img is None:\n            continue\n        \n        # Run YOLO detection on the image\n        results = trained_model.predict(str(img_path), conf=0.3, iou=0.7, verbose=False)[0]\n        if len(results.boxes) > 0:\n            boxes = results.boxes.xyxy.cpu().numpy()  # shape: (N, 4)\n            # Force confidence to 1 for tracker input; alternatively, you can use actual scores.\n            detections = np.hstack([boxes, np.ones((len(boxes), 1))])\n            # Update ByteTrack tracker using image info\n            tracks = byte_tracker.update(\n                output_results=detections,\n                img_info=img.shape[:2],\n                img_size=img.shape[:2]\n            )\n            # Optionally, get associations (if needed)\n            # track_ids = match_detections_with_tracks(boxes, tracks)\n            \n            # Draw each tracked bounding box on the image\n            for trk in tracks:\n                # trk.tlbr should contain [x1, y1, x2, y2]\n                x1, y1, x2, y2 = trk.tlbr\n                cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n                cv2.putText(img, f\"ID: {trk.track_id}\", (int(x1), int(y1)-10),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n        \n        # Write the processed frame to the video\n        video_writer.write(img)\n    \n    video_writer.release()\n    print(f\"âœ… Video saved to {output_video_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"create_tracking_video()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}